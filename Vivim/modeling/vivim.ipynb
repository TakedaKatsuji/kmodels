{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import torch\n",
    "from torch import nn\n",
    "from mamba_ssm import Mamba\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "import math\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\"\n",
    "        channelは最初の次元と最後の次元の二つの形式をサポート。\n",
    "        channels_last : B, H, W, C (default)\n",
    "        channels_first : B, C, H, W\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError\n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[: ,None, None] * x + self.bias[:, None, None]\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1\n",
    "C = 3 \n",
    "H, W = 28, 28\n",
    "x = torch.rand(size=(B, H, W, C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0 mean: 0.0193502027541399 var: 0.9675058183356668\n",
      "C=1 mean: 0.022977638989686966 var: 1.0335289094632714\n",
      "C=2 mean: -0.04232783243060112 var: 0.9998828207013197\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lnorm = LayerNorm(normalized_shape=C)\n",
    "x_norm  = lnorm(x)\n",
    "for i in range(C):\n",
    "    print(f\"C={i}\", \"mean:\", x_norm[..., i].mean().item(), \"var:\", x_norm[..., i].std().item()**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsample conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dwconv = nn.Conv3d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "    \n",
    "    def forward(self, x, nf, H, W):\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1,2).view(B, C, nf, H, W)\n",
    "        x = self.dwconv(x)\n",
    "        x = x.flatten(2).transpose(1,2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5, 224, 224])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 3\n",
    "nf = 5\n",
    "H, W = 224,224\n",
    "B = 1\n",
    "x = torch.randn(B, C, nf, H, W)\n",
    "x = x.flatten(2).transpose(1,2)\n",
    "\n",
    "dwconv = DWConv(dim=C)\n",
    "out = dwconv(x, nf, H, W)\n",
    "out.shape # B, N, C\n",
    "\n",
    "y = out.transpose(1,2).view(B, C, nf, H, W)\n",
    "y.shape # B, C, nf, H, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super(Mlp, self).__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x, nf, H, W):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x, nf, H, W)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2880, 10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 3\n",
    "nf = 5\n",
    "H, W = 24,24\n",
    "B = 1\n",
    "N = nf*H*W\n",
    "x = torch.randn(B, C, nf, H, W)\n",
    "x = x.flatten(2).transpose(1,2) #B, N , C\n",
    "mlp = Mlp(in_features=C, out_features=10)\n",
    "mlp(x, nf, H, W).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MambaLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        d_state=16, d_conv=4, expand=2, mlp_ratio=4, drop=0., drop_path=0., act_layer=nn.GELU\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.mamba = Mamba(\n",
    "            d_model=dim, # モデルの次元数\n",
    "            d_state=d_state, # SSM state expansion factor\n",
    "            expand=expand, # Block expantion factor\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, nf, H, W = x.shape\n",
    "        assert C == self.dim\n",
    "        n_tokens = x.shape[2:].numel()\n",
    "        img_dims = x.shape[2:]\n",
    "        x_flat = x.reshape(B, C, n_tokens).transpose(-1, -2) # B, C, N -> B, N, C\n",
    "        x_mamba = x_flat + self.drop_path(self.mamba(self.norm1(x_flat)))\n",
    "        x_mamba = x_mamba + self.drop_path(self.mlp(self.norm2(x_mamba), nf, H, W))\n",
    "        out = x_mamba.transpose(-1, -2).reshape(B, C, *img_dims)\n",
    "        return out\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5, 28, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "x = torch.randn(1, 3, 5, 28,28)\n",
    "dim = 3\n",
    "mal = MambaLayer(dim=dim).cuda()\n",
    "mal(x.cuda()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mamba_block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone,\n",
    "        in_chans=1,\n",
    "        depths=[2, 2, 2, 2],\n",
    "        dims=[64, 128, 320, 512],\n",
    "        drop_path_rate=0.0,\n",
    "        layer_scale_init_value=1e-6,\n",
    "        out_indices=[0, 1, 2, 3],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.downsample_layers = backbone.segformer.encoder\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        for i in range(len(dims)):\n",
    "            stage = nn.Sequential(\n",
    "                *[\n",
    "                    nn.Sequential(MambaLayer(dim=dims[i], drop_path=dp_rates[i]))\n",
    "                    for j in range(depths[i])\n",
    "                ]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.out_indices = out_indices\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        outs = []\n",
    "        B, nf, C, H, W = x.shape\n",
    "        x = x.reshape(B * nf, x.shape[-3], x.shape[-2], x.shape[-1]) # B*nf, C, H, W\n",
    "\n",
    "        layers = [\n",
    "            self.downsample_layers.patch_embeddings,\n",
    "            self.downsample_layers.block,\n",
    "            self.stages,\n",
    "        ]\n",
    "\n",
    "        for idx, layer in enumerate(zip(*layers)):\n",
    "            embedding_layer, block_layer, mam_stage = layer\n",
    "            # first, obtain patch embeddings\n",
    "            x, height, width = embedding_layer(x)\n",
    "\n",
    "            # second, send embeddings through blocks\n",
    "            for i, blk in enumerate(block_layer):\n",
    "                layer_outputs = blk(x, height, width, False)\n",
    "                x = layer_outputs[0]\n",
    "\n",
    "            # third, optionally reshape back to (batch_size, num_channels, height, width)\n",
    "            x = x.reshape(B * nf, height, width, -1).permute(0, 3, 1, 2).contiguous()\n",
    "            x = x.reshape(B, nf, x.shape[-3], x.shape[-2], x.shape[-1]).transpose(1, 2)\n",
    "            x = mam_stage(x)\n",
    "            x = x.transpose(1, 2)\n",
    "            x = x.reshape(B * nf, x.shape[-3], x.shape[-2], x.shape[-1])\n",
    "\n",
    "            outs.append(x)\n",
    "\n",
    "        return tuple(outs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViVim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vivim(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        out_channels=1,\n",
    "        depths=[2, 2, 2, 2],\n",
    "        feature_size=[64, 128, 320, 512],\n",
    "        drop_path_rate=0,\n",
    "        layer_scale_init_value=1e-6,\n",
    "        hidden_size: int = 768,\n",
    "        spatial_dims=2,\n",
    "        with_edge=False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depths = depths\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "        self.feature_size = feature_size\n",
    "        self.layer_scale_init_value = layer_scale_init_value\n",
    "\n",
    "        self.spatial_dims = spatial_dims\n",
    "\n",
    "        backbone = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            \"nvidia/segformer-b3-finetuned-ade-512-512\"\n",
    "        )\n",
    "        self.encoder = mamba_block(\n",
    "            backbone,\n",
    "            in_channels,\n",
    "            dims=feature_size,\n",
    "        )\n",
    "        self.decoder = backbone.decode_head\n",
    "        # self.decoder.classifier = nn.Sequential()\n",
    "\n",
    "        self.out = nn.Conv2d(768, out_channels, kernel_size=1)\n",
    "        self.with_edge = with_edge\n",
    "        if with_edge:\n",
    "            self.edgeocr_cls_head = nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "\n",
    "    def proj_feat(self, x):\n",
    "        new_view = [x.size(0)] + self.proj_view_shape\n",
    "        x = x.view(new_view)\n",
    "        x = x.permute(self.proj_axes).contiguous()\n",
    "    \n",
    "    def decode(self, encoder_hidden_states):\n",
    "        batch_size = encoder_hidden_states[-1].shape[0]\n",
    "\n",
    "        all_hidden_states = ()\n",
    "        for encoder_hidden_state, mlp in zip(encoder_hidden_states, self.decoder.linear_c):\n",
    "            if self.decoder.config.reshape_last_stage is False and encoder_hidden_state.ndim == 3:\n",
    "                height = width = int(math.sqrt(encoder_hidden_state.shape[-1]))\n",
    "                encoder_hidden_state = (\n",
    "                    encoder_hidden_state.reshape(batch_size, height, width, -1)\n",
    "                    .permute(0, 3, 1, 2)\n",
    "                    .contiguous()\n",
    "                )\n",
    "\n",
    "            # unify channel dimension\n",
    "            height, width = encoder_hidden_state.shape[2], encoder_hidden_state.shape[3]\n",
    "            encoder_hidden_state = mlp(encoder_hidden_state)\n",
    "            encoder_hidden_state = encoder_hidden_state.permute(0, 2, 1)\n",
    "            encoder_hidden_state = encoder_hidden_state.reshape(batch_size, -1, height, width)\n",
    "            # upsample\n",
    "            encoder_hidden_state = nn.functional.interpolate(\n",
    "                encoder_hidden_state,\n",
    "                size=encoder_hidden_states[0].size()[2:],\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            all_hidden_states += (encoder_hidden_state,)\n",
    "        concat_hidden_states = torch.cat(all_hidden_states[::-1], dim=1)\n",
    "        hidden_states = self.decoder.linear_fuse(concat_hidden_states)\n",
    "        hidden_states = self.decoder.batch_norm(hidden_states)\n",
    "        hidden_states = self.decoder.activation(hidden_states)\n",
    "        hidden_states = self.decoder.dropout(hidden_states)\n",
    "\n",
    "        logits = self.out(hidden_states)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, nf, C, H, W = x.shape\n",
    "        outs = self.encoder(x)\n",
    "        logits = self.decode(outs)\n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits, size=(H, W), mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "\n",
    "        if self.with_edge:\n",
    "            edge = self.edgeocr_cls_head(outs[0])\n",
    "            edge = nn.functional.interpolate(edge, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "            return upsampled_logits, edge\n",
    "        else:\n",
    "            return upsampled_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 5, 1, 224, 224).cuda()\n",
    "model = Vivim(in_channels=1, out_channels=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x).shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vivim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
