{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import torch\n",
    "from torch import nn\n",
    "from mamba_ssm import Mamba\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "import math\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\"\n",
    "        channelは最初の次元と最後の次元の二つの形式をサポート。\n",
    "        channels_last : B, H, W, C (default)\n",
    "        channels_first : B, C, H, W\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError\n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[: ,None, None] * x + self.bias[:, None, None]\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1\n",
    "C = 3 \n",
    "H, W = 28, 28\n",
    "x = torch.rand(size=(B, H, W, C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0 mean: -0.004667768720537424 var: 1.000094654415662\n",
      "C=1 mean: 0.006295911036431789 var: 1.0178018509495956\n",
      "C=2 mean: -0.0016281544230878353 var: 0.9855169191951063\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lnorm = LayerNorm(normalized_shape=C)\n",
    "x_norm  = lnorm(x)\n",
    "for i in range(C):\n",
    "    print(f\"C={i}\", \"mean:\", x_norm[..., i].mean().item(), \"var:\", x_norm[..., i].std().item()**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsample conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dwconv = nn.Conv3d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "    \n",
    "    def forward(self, x, nf, H, W):\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1,2).view(B, C, nf, H, W)\n",
    "        x = self.dwconv(x)\n",
    "        x = x.flatten(2).transpose(1,2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5, 224, 224])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 3\n",
    "nf = 5\n",
    "H, W = 224,224\n",
    "B = 1\n",
    "x = torch.randn(B, C, nf, H, W)\n",
    "x = x.flatten(2).transpose(1,2)\n",
    "\n",
    "dwconv = DWConv(dim=C)\n",
    "out = dwconv(x, nf, H, W)\n",
    "out.shape # B, N, C\n",
    "\n",
    "y = out.transpose(1,2).view(B, C, nf, H, W)\n",
    "y.shape # B, C, nf, H, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super(Mlp, self).__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x, nf, H, W):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x, nf, H, W)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2880, 10])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 3\n",
    "nf = 5\n",
    "H, W = 24,24\n",
    "B = 1\n",
    "N = nf*H*W\n",
    "x = torch.randn(B, C, nf, H, W)\n",
    "x = x.flatten(2).transpose(1,2) #B, N , C\n",
    "mlp = Mlp(in_features=C, out_features=10)\n",
    "mlp(x, nf, H, W).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MambaLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        d_state=16, d_conv=4, expand=2, mlp_ratio=4, drop=0., drop_path=0., act_layer=nn.GELU\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.mamba = Mamba(\n",
    "            d_model=dim, # モデルの次元数\n",
    "            d_state=d_state, # SSM state expansion factor\n",
    "            expand=expand, # Block expantion factor\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, nf, H, W = x.shape\n",
    "        assert C == self.dim\n",
    "        n_tokens = x.shape[2:].numel()\n",
    "        img_dims = x.shape[2:]\n",
    "        x_flat = x.reshape(B, C, n_tokens).transpose(-1, -2) # B, C, N -> B, N, C\n",
    "        x_mamba = x_flat + self.drop_path(self.mamba(self.norm1(x_flat)))\n",
    "        x_mamba = x_mamba + self.drop_path(self.mlp(self.norm2(x_mamba), nf, H, W))\n",
    "        out = x_mamba.transpose(-1, -2).reshape(B, C, *img_dims)\n",
    "        return out\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5, 28, 28])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "x = torch.randn(1, 3, 5, 28,28)\n",
    "dim = 3\n",
    "mal = MambaLayer(dim=dim).cuda()\n",
    "mal(x.cuda()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mamba_block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone,\n",
    "        in_chans=1,\n",
    "        depths=[2, 2, 2, 2],\n",
    "        dims=[64, 128, 320, 512],\n",
    "        drop_path_rate=0.0,\n",
    "        layer_scale_init_value=1e-6,\n",
    "        out_indices=[0, 1, 2, 3],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.downsample_layers = backbone.segformer.encoder\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        for i in range(len(dims)):\n",
    "            stage = nn.Sequential(\n",
    "                *[\n",
    "                    nn.Sequential(MambaLayer(dim=dims[i], drop_path=dp_rates[i]))\n",
    "                    for j in range(depths[i])\n",
    "                ]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.out_indices = out_indices\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        outs = []\n",
    "        B, nf, C, H, W = x.shape\n",
    "        x = x.reshape(B * nf, x.shape[-3], x.shape[-2], x.shape[-1]) # B*nf, C, H, W\n",
    "\n",
    "        layers = [\n",
    "            self.downsample_layers.patch_embeddings,\n",
    "            self.downsample_layers.block,\n",
    "            self.stages,\n",
    "        ]\n",
    "\n",
    "        for idx, layer in enumerate(zip(*layers)):\n",
    "            embedding_layer, block_layer, mam_stage = layer\n",
    "            # first, obtain patch embeddings\n",
    "            x, height, width = embedding_layer(x)\n",
    "\n",
    "            # second, send embeddings through blocks\n",
    "            for i, blk in enumerate(block_layer):\n",
    "                layer_outputs = blk(x, height, width, False)\n",
    "                x = layer_outputs[0]\n",
    "\n",
    "            # third, optionally reshape back to (batch_size, num_channels, height, width)\n",
    "            x = x.reshape(B * nf, height, width, -1).permute(0, 3, 1, 2).contiguous()\n",
    "            x = x.reshape(B, nf, x.shape[-3], x.shape[-2], x.shape[-1]).transpose(1, 2)\n",
    "            x = mam_stage(x)\n",
    "            x = x.transpose(1, 2)\n",
    "            x = x.reshape(B * nf, x.shape[-3], x.shape[-2], x.shape[-1])\n",
    "\n",
    "            outs.append(x)\n",
    "\n",
    "        return tuple(outs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViVim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vivim(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        out_channels=1,\n",
    "        depths=[2, 2, 2, 2],\n",
    "        feature_size=[64, 128, 320, 512],\n",
    "        drop_path_rate=0,\n",
    "        layer_scale_init_value=1e-6,\n",
    "        hidden_size: int = 768,\n",
    "        spatial_dims=2,\n",
    "        with_edge=False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depths = depths\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "        self.feature_size = feature_size\n",
    "        self.layer_scale_init_value = layer_scale_init_value\n",
    "\n",
    "        self.spatial_dims = spatial_dims\n",
    "\n",
    "        backbone = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            \"nvidia/segformer-b3-finetuned-ade-512-512\"\n",
    "        )\n",
    "        self.encoder = mamba_block(\n",
    "            backbone,\n",
    "            in_channels,\n",
    "            dims=feature_size,\n",
    "        )\n",
    "        self.decoder = backbone.decode_head\n",
    "        # self.decoder.classifier = nn.Sequential()\n",
    "\n",
    "        self.out = nn.Conv2d(768, out_channels, kernel_size=1)\n",
    "        self.with_edge = with_edge\n",
    "        if with_edge:\n",
    "            self.edgeocr_cls_head = nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "\n",
    "    def proj_feat(self, x):\n",
    "        new_view = [x.size(0)] + self.proj_view_shape\n",
    "        x = x.view(new_view)\n",
    "        x = x.permute(self.proj_axes).contiguous()\n",
    "    \n",
    "    def decode(self, encoder_hidden_states):\n",
    "        batch_size = encoder_hidden_states[-1].shape[0]\n",
    "\n",
    "        all_hidden_states = ()\n",
    "        for encoder_hidden_state, mlp in zip(encoder_hidden_states, self.decoder.linear_c):\n",
    "            if self.decoder.config.reshape_last_stage is False and encoder_hidden_state.ndim == 3:\n",
    "                height = width = int(math.sqrt(encoder_hidden_state.shape[-1]))\n",
    "                encoder_hidden_state = (\n",
    "                    encoder_hidden_state.reshape(batch_size, height, width, -1)\n",
    "                    .permute(0, 3, 1, 2)\n",
    "                    .contiguous()\n",
    "                )\n",
    "\n",
    "            # unify channel dimension\n",
    "            height, width = encoder_hidden_state.shape[2], encoder_hidden_state.shape[3]\n",
    "            encoder_hidden_state = mlp(encoder_hidden_state)\n",
    "            encoder_hidden_state = encoder_hidden_state.permute(0, 2, 1)\n",
    "            encoder_hidden_state = encoder_hidden_state.reshape(batch_size, -1, height, width)\n",
    "            # upsample\n",
    "            encoder_hidden_state = nn.functional.interpolate(\n",
    "                encoder_hidden_state,\n",
    "                size=encoder_hidden_states[0].size()[2:],\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            all_hidden_states += (encoder_hidden_state,)\n",
    "        concat_hidden_states = torch.cat(all_hidden_states[::-1], dim=1)\n",
    "        hidden_states = self.decoder.linear_fuse(concat_hidden_states)\n",
    "        hidden_states = self.decoder.batch_norm(hidden_states)\n",
    "        hidden_states = self.decoder.activation(hidden_states)\n",
    "        hidden_states = self.decoder.dropout(hidden_states)\n",
    "\n",
    "        logits = self.out(hidden_states)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, nf, C, H, W = x.shape\n",
    "        outs = self.encoder(x)\n",
    "        logits = self.decode(outs)\n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits, size=(H, W), mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "\n",
    "        if self.with_edge:\n",
    "            edge = self.edgeocr_cls_head(outs[0])\n",
    "            edge = nn.functional.interpolate(edge, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "            return upsampled_logits, edge\n",
    "        else:\n",
    "            return upsampled_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 5, 3, 224, 224).cuda()\n",
    "model = Vivim(in_channels=3, out_channels=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 224, 224])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Mamba.__init__() got an unexpected keyword argument 'bimamba_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[244], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvivim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vivim\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mVivim\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      4\u001b[0m model(x)\n",
      "File \u001b[0;32m~/project/study/kmodels/Vivim/modeling/vivim.py:293\u001b[0m, in \u001b[0;36mVivim.__init__\u001b[0;34m(self, in_chans, out_chans, depths, feat_size, drop_path_rate, layer_scale_init_value, hidden_size, norm_name, conv_block, res_block, spatial_dims, with_edge)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_dims \u001b[38;5;241m=\u001b[39m spatial_dims\n\u001b[1;32m    290\u001b[0m backbone \u001b[38;5;241m=\u001b[39m SegformerForSemanticSegmentation\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia/segformer-b3-finetuned-ade-512-512\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mmamba_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_chans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeat_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m backbone\u001b[38;5;241m.\u001b[39mdecode_head\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# self.decoder.classifier = nn.Sequential()\u001b[39;00m\n",
      "File \u001b[0;32m~/project/study/kmodels/Vivim/modeling/vivim.py:204\u001b[0m, in \u001b[0;36mmamba_block.__init__\u001b[0;34m(self, backbone, in_chans, depths, dims, drop_path_rate, layer_scale_init_value, out_indices)\u001b[0m\n\u001b[1;32m    201\u001b[0m cur \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dims)):\n\u001b[1;32m    203\u001b[0m     stage \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    205\u001b[0m             nn\u001b[38;5;241m.\u001b[39mSequential(MambaLayer(dim\u001b[38;5;241m=\u001b[39mdims[i], drop_path\u001b[38;5;241m=\u001b[39mdp_rates[i]))\n\u001b[1;32m    206\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depths[i])\n\u001b[1;32m    207\u001b[0m         ]\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages\u001b[38;5;241m.\u001b[39mappend(stage)\n\u001b[1;32m    210\u001b[0m     cur \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m depths[i]\n",
      "File \u001b[0;32m~/project/study/kmodels/Vivim/modeling/vivim.py:205\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    201\u001b[0m cur \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dims)):\n\u001b[1;32m    203\u001b[0m     stage \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m--> 205\u001b[0m             nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[43mMambaLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdp_rates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    206\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depths[i])\n\u001b[1;32m    207\u001b[0m         ]\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages\u001b[38;5;241m.\u001b[39mappend(stage)\n\u001b[1;32m    210\u001b[0m     cur \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m depths[i]\n",
      "File \u001b[0;32m~/project/study/kmodels/Vivim/modeling/vivim.py:127\u001b[0m, in \u001b[0;36mMambaLayer.__init__\u001b[0;34m(self, dim, d_state, d_conv, expand, mlp_ratio, drop, drop_path, act_layer)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m=\u001b[39m dim\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(dim)\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmamba \u001b[38;5;241m=\u001b[39m \u001b[43mMamba\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Model dimension d_model\u001b[39;49;00m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# SSM state expansion factor\u001b[39;49;00m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_conv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Local convolution width\u001b[39;49;00m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Block expansion factor\u001b[39;49;00m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbimamba_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# use_fast_path=False,\u001b[39;49;00m\n\u001b[1;32m    134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path \u001b[38;5;241m=\u001b[39m DropPath(drop_path) \u001b[38;5;28;01mif\u001b[39;00m drop_path \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(dim)\n",
      "\u001b[0;31mTypeError\u001b[0m: Mamba.__init__() got an unexpected keyword argument 'bimamba_type'"
     ]
    }
   ],
   "source": [
    "from vivim import Vivim\n",
    "model = Vivim(in_chans=1, out_chans=1).cuda()\n",
    "x = torch.randn(1, 5, 1, 224, 224).cuda()\n",
    "model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vivim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
